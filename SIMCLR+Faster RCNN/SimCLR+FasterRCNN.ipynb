{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimCLR+FasterRCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF8ZoVrwt0n0",
        "colab_type": "text"
      },
      "source": [
        "# SimCLR\n",
        "PyTorch implementation of SimCLR: A Simple Framework for Contrastive Learning of Visual Representations by T. Chen et al. With support for the LARS (Layer-wise Adaptive Rate Scaling) optimizer.\n",
        "\n",
        "[Link to paper](https://arxiv.org/pdf/2002.05709.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lt6WMxjCvN3o",
        "colab_type": "text"
      },
      "source": [
        "## Setup the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53JMIYtat8tT",
        "colab_type": "code",
        "outputId": "dc9bc80b-0932-49fc-859e-c9cf2ae994d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#!git clone https://github.com/AmeerHamza111/SimCLR.git\n",
        "%cd SimCLR\n",
        "#!wget https://github.com/Spijkervet/SimCLR/releases/download/1.2/checkpoint_100.tar\n",
        "#!sh setup.sh || python3 -m pip install -r requirements.txt || exit 1\n",
        "#!pip install  pyyaml --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/SimCLR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7g9kN-48Szp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fe4400e2-d135-4019-b9a5-0305a0f0f1a1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/data/')\n",
        "from pathlib import Path\n",
        "base_dir = ('/data/My Drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /data/; to attempt to forcibly remount, call drive.mount(\"/data/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmQcxjxk70qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!cp  /data/My\\ Drive/DeepLearning/student_data.zip /content/SimCLR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm3jKnfL8vii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip student_data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ3jq3cWynLf",
        "colab_type": "text"
      },
      "source": [
        "# Part 1:\n",
        "## SimCLR pre-training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jhAv3hv8IHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# whether to use a TPU or not (set in Runtime -> Change Runtime Type)\n",
        "use_tpu = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwW10d2O7pn8",
        "colab_type": "text"
      },
      "source": [
        "#### Install PyTorch/XLA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj84aiC27oxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_tpu:\n",
        "  VERSION = \"20200220\" #@param [\"20200220\",\"nightly\", \"xrt==1.15.0\"]\n",
        "  !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "  !python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNDRcPbbymlX",
        "colab_type": "code",
        "outputId": "d5a12c43-8a47-4679-bbd1-87525802d58f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "if use_tpu:\n",
        "  # imports the torch_xla package for TPU support\n",
        "  import torch_xla\n",
        "  import torch_xla.core.xla_model as xm\n",
        "  dev = xm.xla_device()\n",
        "  print(dev)\n",
        "  \n",
        "import torchvision\n",
        "import argparse\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "apex = False\n",
        "try:\n",
        "    from apex import amp\n",
        "    apex = True\n",
        "except ImportError:\n",
        "    print(\n",
        "        \"Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\"\n",
        "    )\n",
        "\n",
        "from model import load_model, save_model\n",
        "from modules import NT_Xent\n",
        "from modules.transformations import TransformsSimCLR\n",
        "from utils import mask_correlated_samples, post_config_hook\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Install the apex package from https://www.github.com/nvidia/apex to use fp16 for training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af1M5CnEAQky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install sacred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abk6aFZxyedW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, train_loader, model, criterion, optimizer, writer):\n",
        "    loss_epoch = 0\n",
        "    for step, ((x_i, x_j), _) in enumerate(train_loader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        x_i = x_i.to(args.device)\n",
        "        x_j = x_j.to(args.device)\n",
        "\n",
        "        # positive pair, with encoding\n",
        "        h_i, z_i = model(x_i)\n",
        "        h_j, z_j = model(x_j)\n",
        "\n",
        "        loss = criterion(z_i, z_j)\n",
        "\n",
        "        if apex and args.fp16:\n",
        "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                scaled_loss.backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"Step [{step}/{len(train_loader)}]\\t Loss: {loss.item()}\")\n",
        "\n",
        "        writer.add_scalar(\"Loss/train_epoch\", loss.item(), args.global_step)\n",
        "        loss_epoch += loss.item()\n",
        "        args.global_step += 1\n",
        "\n",
        "    return loss_epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYbV0fa_y03Z",
        "colab_type": "text"
      },
      "source": [
        "### Load arguments from `config/config.yaml`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1klUf-IuyxdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "from utils.yaml_config_hook import yaml_config_hook\n",
        "\n",
        "config = yaml_config_hook(\"./config/config.yaml\")\n",
        "args = argparse.Namespace(**config)\n",
        "\n",
        "if use_tpu:\n",
        "  args.device = dev\n",
        "else:\n",
        "  args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  \n",
        "args.out_dir = \"logs\"\n",
        "if not os.path.exists(\"logs\"):\n",
        "  os.makedirs(\"logs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O86__UhA0Lvr",
        "colab_type": "code",
        "outputId": "156d158c-2204-4a14-a5ba-a66cb2fda833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "args.batch_size = 2\n",
        "args.epochs = 1\n",
        "args.epoch_num = 1\n",
        "args.resnet = \"resnet18\"\n",
        "args.dataset = \"road\"\n",
        "args.model_path = \"logs/simclrPretext\"\n",
        "#args.optimizer = \"LARS\"\n",
        "pprint(vars(args))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 2,\n",
            " 'dataset': 'road',\n",
            " 'device': device(type='cuda', index=0),\n",
            " 'epoch_num': 1,\n",
            " 'epochs': 1,\n",
            " 'fp16': False,\n",
            " 'fp16_opt_level': 'O2',\n",
            " 'logistic_batch_size': 256,\n",
            " 'logistic_epochs': 100,\n",
            " 'model_path': 'logs/simclrPretext',\n",
            " 'normalize': True,\n",
            " 'optimizer': 'Adam',\n",
            " 'out_dir': 'logs',\n",
            " 'projection_dim': 64,\n",
            " 'resnet': 'resnet18',\n",
            " 'seed': 42,\n",
            " 'start_epoch': 0,\n",
            " 'temperature': 0.5,\n",
            " 'weight_decay': 1e-06,\n",
            " 'workers': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4s4vzwO-fnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_folder = 'data'\n",
        "annotation_csv = 'data/annotation.csv'\n",
        "\n",
        "import numpy as np\n",
        "from data_helper import SimclrUnlabeledDataset\n",
        "from helper import convert_map_to_lane_map, convert_map_to_road_map, collate_fn, draw_box\n",
        "\n",
        "unlabeled_scene_index = np.arange(106)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJfeOM9PzNoF",
        "colab_type": "text"
      },
      "source": [
        "### Load dataset into train loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGcskdBsytbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"./datasets\"\n",
        "\n",
        "train_sampler = None\n",
        "\n",
        "if args.dataset == \"STL10\":\n",
        "    train_dataset = torchvision.datasets.STL10(\n",
        "        root, split=\"unlabeled\", download=True, transform=TransformsSimCLR()\n",
        "    )\n",
        "elif args.dataset == \"CIFAR10\":\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root, download=True, transform=TransformsSimCLR()\n",
        "    )\n",
        "elif args.dataset == \"road\":\n",
        "    train_dataset = SimclrUnlabeledDataset(image_folder=image_folder, \n",
        "      scene_index=unlabeled_scene_index, first_dim='sample', transform=TransformsSimCLR())\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.batch_size,\n",
        "    shuffle=(train_sampler is None),\n",
        "    drop_last=True,\n",
        "    num_workers=16,\n",
        "    sampler=train_sampler,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBlXZwvjzPmp",
        "colab_type": "text"
      },
      "source": [
        "### Load the SimCLR model, optimizer and learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xERq_yHSzJRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, optimizer, scheduler = load_model(args, train_loader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htNZNjbtDlBN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca4d75df-5c68-41ae-e17f-4fa2b544054d"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SimCLR(\n",
            "  (encoder): ResNet(\n",
            "    (conv1): Conv2d(18, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (projector): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=512, bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=64, bias=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyJ3ulWqzViL",
        "colab_type": "text"
      },
      "source": [
        "### Setup TensorBoard for logging experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZNieMqfzU7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tb_dir = os.path.join(args.out_dir, \"colab\")\n",
        "if not os.path.exists(tb_dir):\n",
        "  os.makedirs(tb_dir)\n",
        "writer = SummaryWriter(log_dir=tb_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpl6uQiIzbvK",
        "colab_type": "text"
      },
      "source": [
        "### Create the mask that will remove correlated samples from the negative examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMHQYxC2Cc2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mask = mask_correlated_samples(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtNCVEynzjtV",
        "colab_type": "text"
      },
      "source": [
        "### Initialize the criterion (NT-Xent loss)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u067AY93zh-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = NT_Xent(args.batch_size, args.temperature, mask, args.device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN5KBK-yztGD",
        "colab_type": "text"
      },
      "source": [
        "### Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdCrD62hzjDQ",
        "colab_type": "code",
        "outputId": "593f2f19-93eb-4a57-e455-42009d42a084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''args.global_step = 0\n",
        "args.current_epoch = 0\n",
        "for epoch in range(args.start_epoch, args.epochs):\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    loss_epoch = train(args, train_loader, model, criterion, optimizer, writer)\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step()\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        save_model(args, model, optimizer)\n",
        "\n",
        "    writer.add_scalar(\"Loss/train\", loss_epoch / len(train_loader), epoch)\n",
        "    writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\n",
        "    print(\n",
        "        f\"Epoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\"\n",
        "    )\n",
        "    args.current_epoch += 1\n",
        "\n",
        "## end training\n",
        "save_model(args, model, optimizer)'''"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'args.global_step = 0\\nargs.current_epoch = 0\\nfor epoch in range(args.start_epoch, args.epochs):\\n    lr = optimizer.param_groups[0][\\'lr\\']\\n    loss_epoch = train(args, train_loader, model, criterion, optimizer, writer)\\n\\n    if scheduler:\\n        scheduler.step()\\n\\n    if epoch % 5 == 0:\\n        save_model(args, model, optimizer)\\n\\n    writer.add_scalar(\"Loss/train\", loss_epoch / len(train_loader), epoch)\\n    writer.add_scalar(\"Misc/learning_rate\", lr, epoch)\\n    print(\\n        f\"Epoch [{epoch}/{args.epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t lr: {round(lr, 5)}\"\\n    )\\n    args.current_epoch += 1\\n\\n## end training\\nsave_model(args, model, optimizer)'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77BXUR9_4hNc",
        "colab_type": "text"
      },
      "source": [
        "## Download last checkpoint to local drive (replace `100` with `args.epochs`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7eHATk04Sgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#files.download('./logs/checkpoint_100.tar')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc7wOlflIZn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_bb_loss(boxes1, boxes2):\n",
        "    boxes1 = convertBoundingBoxes(boxes1)\n",
        "    boxes2 = convertBoundingBoxes(boxes2)\n",
        "    \n",
        "    num_boxes1 = boxes1.size(0)\n",
        "    num_boxes2 = boxes2.size(0)\n",
        "\n",
        "    boxes1_max_x = boxes1[:, 0].max(dim=1)[0]\n",
        "    boxes1_min_x = boxes1[:, 0].min(dim=1)[0]\n",
        "    boxes1_max_y = boxes1[:, 1].max(dim=1)[0]\n",
        "    boxes1_min_y = boxes1[:, 1].min(dim=1)[0]\n",
        "\n",
        "    boxes2_max_x = boxes2[:, 0].max(dim=1)[0]\n",
        "    boxes2_min_x = boxes2[:, 0].min(dim=1)[0]\n",
        "    boxes2_max_y = boxes2[:, 1].max(dim=1)[0]\n",
        "    boxes2_min_y = boxes2[:, 1].min(dim=1)[0]\n",
        "\n",
        "    condition1_matrix = (boxes1_max_x.unsqueeze(1) > boxes2_min_x.unsqueeze(0))\n",
        "    condition2_matrix = (boxes1_min_x.unsqueeze(1) < boxes2_max_x.unsqueeze(0))\n",
        "    condition3_matrix = (boxes1_max_y.unsqueeze(1) > boxes2_min_y.unsqueeze(0))\n",
        "    condition4_matrix = (boxes1_min_y.unsqueeze(1) < boxes2_max_y.unsqueeze(0))\n",
        "    condition_matrix = condition1_matrix * condition2_matrix * condition3_matrix * condition4_matrix\n",
        "\n",
        "    iou_matrix = torch.zeros(num_boxes1, num_boxes2)\n",
        "    k = 0\n",
        "    loss = 0\n",
        "    for i in range(num_boxes1):\n",
        "        for j in range(num_boxes2):\n",
        "            if condition_matrix[i][j]:\n",
        "                iou_matrix[i][j] = compute_iou(boxes1[i], boxes2[j])\n",
        "                loss += - torch.log(iou_matrix[i][[j]])\n",
        "                k += 1\n",
        "    return loss/k\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    a = Polygon(torch.t(box1)).convex_hull\n",
        "    b = Polygon(torch.t(box2)).convex_hull\n",
        "    \n",
        "    return a.intersection(b).area / a.union(b).area\n",
        "\n",
        "def convertBoundingBoxes(boxes):\n",
        "    convBoxes = []\n",
        "    for box in boxes: \n",
        "        xmin = box[0]\n",
        "        ymin = box[1]\n",
        "        xmax = box[2]\n",
        "        ymax = box[3]\n",
        "        cbox = [[xmin,xmin,xmax,xmax], [ymin,ymax,ymin,ymax]]\n",
        "        convBoxes.append(cbox)\n",
        "        convBoxes = torch.Tensor(convBoxes)\n",
        "        return convBoxes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUX58m5dGr4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3feb1940-f46c-4e08-8fba-50714b6bd393"
      },
      "source": [
        "from modelobj import NetObj\n",
        "mdl = NetObj()\n",
        "model1 = mdl.model\n",
        "model1.to(args.device)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d()\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d()\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d()\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d()\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d()\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign()\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=9, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBdSoqw2Hkrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "model, _, _ = load_model(args, train_loader, reload_model=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhDOBHGaJa4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model2 = nn.Sequential(model.encoder.conv1, model.encoder.bn1,model.encoder.relu, model.encoder.maxpool,\n",
        "                      model.encoder.layer1, model.encoder.layer2, model.encoder.layer3, model.encoder.layer4[0])\n",
        "\n",
        "model3 = nn.Sequential(model.encoder.layer4[1].conv1,model.encoder.layer4[1].bn1, nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),                     \n",
        "nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),model1.backbone.fpn.extra_blocks,model1.rpn, model1.roi_heads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpNlsevNS5mF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "754cc47f-25ea-4fce-c7be-9fc42afb2535"
      },
      "source": [
        "model2"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(18, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): ReLU(inplace=True)\n",
              "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (5): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (6): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (7): BasicBlock(\n",
              "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (downsample): Sequential(\n",
              "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LtbitAWTqRy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "60771c84-3754-457c-afaa-5740cec2b198"
      },
      "source": [
        "model3"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (4): LastLevelMaxPool()\n",
              "  (5): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (6): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign()\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=9, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX-_gTpjVtfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from engine import train_one_epoch, evaluate\n",
        "#import utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uDbGhHmW2m1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQpjiuJy61N",
        "colab_type": "text"
      },
      "source": [
        "# Part 2:\n",
        "## Linear evaluation using logistic regression, using weights from frozen, pre-trained SimCLR model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24wrzMP2vYcV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFyS9RvpuCuC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import argparse\n",
        "\n",
        "from experiment import ex\n",
        "from model import load_model\n",
        "from utils import post_config_hook\n",
        "\n",
        "#from modules import LogisticRegression\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qoR15jYVsGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZRtPBCLvgqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, loader, model2, model3, criterion, optimizer):\n",
        "    loss_epoch = 0\n",
        "    accuracy_epoch = 0\n",
        "    model2.to(args.device)\n",
        "    model3.to(args.device)\n",
        "    model2.eval()\n",
        "    model3.train()\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #print(x)\n",
        "        print(y)\n",
        "        #print(y.shape)\n",
        "\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "\n",
        "        # get encoding\n",
        "        with torch.no_grad():\n",
        "            h = model2(x)\n",
        "            print(h.shape)\n",
        "            # h = 512\n",
        "            # z = 64\n",
        "\n",
        "        output = model3(h[0],y)\n",
        "        print(output)\n",
        "        loss = compute_bb_loss(output['boxes'], y)\n",
        "\n",
        "        '''predicted = output.argmax(1)\n",
        "        acc = (predicted == y).sum().item() / y.size(0)\n",
        "        accuracy_epoch += acc'''\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "        if step % 1 == 0:\n",
        "            print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()}\\t Accuracy: 0\")\n",
        "\n",
        "    return loss_epoch, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skBYAPb2uKB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, loader, model2, model3, model, criterion, optimizer):\n",
        "    loss_epoch = 0\n",
        "    accuracy_epoch = 0\n",
        "    model2.to(args.device)\n",
        "    model3.to(args.device)\n",
        "    model2.eval()\n",
        "    model3.eval()\n",
        "    for step, (x, y) in enumerate(loader):\n",
        "        model.zero_grad()\n",
        "\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "\n",
        "        # get encoding\n",
        "        with torch.no_grad():\n",
        "            h = model2(x)\n",
        "            # h = 512\n",
        "            # z = 64\n",
        "\n",
        "        output = model3(h)\n",
        "        loss = compute_bb_loss(output['boxes'], y)\n",
        "\n",
        "        '''predicted = output.argmax(1)\n",
        "        acc = (predicted == y).sum().item() / y.size(0)\n",
        "        accuracy_epoch += acc'''\n",
        "\n",
        "        loss_epoch += loss.item()\n",
        "\n",
        "\n",
        "    return loss_epoch, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJk4-nc-vkF0",
        "colab_type": "code",
        "outputId": "fe4d36a0-aba1-4e1c-ae64-9e2f3aba9724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "from pprint import pprint\n",
        "from utils.yaml_config_hook import yaml_config_hook\n",
        "\n",
        "config = yaml_config_hook(\"./config/config.yaml\")\n",
        "pprint(config)\n",
        "args = argparse.Namespace(**config)\n",
        "\n",
        "if use_tpu:\n",
        "  args.device = dev\n",
        "else:\n",
        "  args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 128,\n",
            " 'dataset': 'STL10',\n",
            " 'epoch_num': 100,\n",
            " 'epochs': 100,\n",
            " 'fp16': False,\n",
            " 'fp16_opt_level': 'O2',\n",
            " 'logistic_batch_size': 256,\n",
            " 'logistic_epochs': 100,\n",
            " 'model_path': 'logs/0',\n",
            " 'normalize': True,\n",
            " 'optimizer': 'Adam',\n",
            " 'projection_dim': 64,\n",
            " 'resnet': 'resnet50',\n",
            " 'seed': 42,\n",
            " 'start_epoch': 0,\n",
            " 'temperature': 0.5,\n",
            " 'weight_decay': 1e-06,\n",
            " 'workers': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7cSwhu55KJc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "cbe91ec5-b0e2-4075-b97f-3896fc70341a"
      },
      "source": [
        "args.batch_size = 2\n",
        "args.resnet = \"resnet18\"\n",
        "args.model_path = \"logs/simclrPretext\"\n",
        "args.epochs = 1\n",
        "args.epoch_num = 1\n",
        "args.dataset = 'road'\n",
        "args.logistic_epochs =1\n",
        "args.logistic_batch_size = 1\n",
        "args.out_dir = \"logs/train\"\n",
        "#args.optimizer = \"LARS\"\n",
        "pprint(vars(args))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 2,\n",
            " 'dataset': 'road',\n",
            " 'device': device(type='cuda', index=0),\n",
            " 'epoch_num': 1,\n",
            " 'epochs': 1,\n",
            " 'fp16': False,\n",
            " 'fp16_opt_level': 'O2',\n",
            " 'logistic_batch_size': 1,\n",
            " 'logistic_epochs': 1,\n",
            " 'model_path': 'logs/simclrPretext',\n",
            " 'normalize': True,\n",
            " 'optimizer': 'Adam',\n",
            " 'out_dir': 'logs/train',\n",
            " 'projection_dim': 64,\n",
            " 'resnet': 'resnet18',\n",
            " 'seed': 42,\n",
            " 'start_epoch': 0,\n",
            " 'temperature': 0.5,\n",
            " 'weight_decay': 1e-06,\n",
            " 'workers': 16}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02CLdHAqRSYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labeled_scene_index = np.arange(106, 120)\n",
        "\n",
        "## validation scene index.\n",
        "validation_scene_index = np.arange(120, 134)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2d3BlaNRVUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transform = torchvision.transforms.Compose(\n",
        "            [\n",
        "                #torchvision.transforms.RandomResizedCrop(size=(256,306)),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "            ]\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfDix_tSRWwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from data_helper import SimclrLabeledBBDataset, SimclrLabeledDataset, ObjDetectionLabeledDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCkssHpPeosY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from helper import collate_fn, draw_box"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWRuVrZZ5Vm1",
        "colab_type": "text"
      },
      "source": [
        "### Load dataset into train/test dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPGuFjLW5PF9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root = \"./datasets\"\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "if args.dataset == \"STL10\":\n",
        "    train_dataset = torchvision.datasets.STL10(\n",
        "        root, split=\"train\", download=True, transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.STL10(\n",
        "        root, split=\"test\", download=True, transform=torchvision.transforms.ToTensor()\n",
        "    )\n",
        "elif args.dataset == \"CIFAR10\":\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root, train=True, download=True, transform=transform\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root, train=False, download=True, transform=transform\n",
        "    )\n",
        "elif args.dataset == \"road\":\n",
        "    train_dataset =  SimclrLabeledBBDataset(image_folder=image_folder,\n",
        "                                  annotation_file=annotation_csv,\n",
        "                                  scene_index=labeled_scene_index,\n",
        "                                  transform=transform,\n",
        "                                  extra_info=False\n",
        "                                 )\n",
        "    test_dataset = SimclrLabeledBBDataset(image_folder=image_folder,\n",
        "                                  annotation_file=annotation_csv,\n",
        "                                  scene_index=validation_scene_index,\n",
        "                                  transform=transform,\n",
        "                                  extra_info=False\n",
        "                                 )\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=args.logistic_batch_size,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=args.logistic_batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=4\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QmLCIPZVvhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#a,b = iter(train_loader).next()\n",
        "#print(a)\n",
        "#print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjlUv9_8ZaMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw0cMiayZMo6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cca7ed22-6a96-4028-bd84-af15db96f44a"
      },
      "source": [
        "'''for epoch in range(0, 5):\n",
        "  # train for one epoch, printing every 10 iterations\n",
        "  train_one_epoch(model2, model3, optimizer, train_loader, args.device, epoch, print_freq=10)\n",
        "  # update the learning rate\n",
        "  lr_scheduler.step()\n",
        "  # evaluate on the test dataset\n",
        "  evaluate(model2, model3, test_loader, device=args.device)\n",
        "\n",
        "  model_file = 'modobj/fasterrcnn_model_' + str(epoch) + '.pth'\n",
        "  torch.save({'modelObjectDetection_state_dict': model.state_dict()},model_file)\n",
        "  print('\\nSaved model to ' + model_file )'''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"for epoch in range(0, 5):\\n  # train for one epoch, printing every 10 iterations\\n  train_one_epoch(model2, model3, optimizer, train_loader, args.device, epoch, print_freq=10)\\n  # update the learning rate\\n  lr_scheduler.step()\\n  # evaluate on the test dataset\\n  evaluate(model2, model3, test_loader, device=args.device)\\n\\n  model_file = 'modobj/fasterrcnn_model_' + str(epoch) + '.pth'\\n  torch.save({'modelObjectDetection_state_dict': model.state_dict()},model_file)\\n  print('\\nSaved model to ' + model_file )\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmwXqVBH5ZX6",
        "colab_type": "text"
      },
      "source": [
        "### Load SimCLR model and load model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTVnvx2a5QnX",
        "colab_type": "code",
        "outputId": "ccb1966a-6b8c-430b-dca7-8967c7607cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''simclr_model, _, _ = load_model(args, train_loader, reload_model=True)\n",
        "simclr_model = simclr_model.to(args.device)\n",
        "simclr_model.eval()'''"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'simclr_model, _, _ = load_model(args, train_loader, reload_model=True)\\nsimclr_model = simclr_model.to(args.device)\\nsimclr_model.eval()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZoABGRr5Q8_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "843aae36-f304-4ff8-d410-91a6bbdc7ace"
      },
      "source": [
        "## Logistic Regression\n",
        "''''n_classes = 9 # stl-10\n",
        "model = LogisticRegression(simclr_model.n_features, n_classes)\n",
        "model = model.to(args.device)'''"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'n_classes = 9 # stl-10\\nmodel = LogisticRegression(simclr_model.n_features, n_classes)\\nmodel = model.to(args.device)\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T694n_HQ5Tad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLaebM9Qvztx",
        "colab_type": "code",
        "outputId": "64990264-6dc7-4796-98c0-72e8ce9b3558",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "for epoch in range(args.logistic_epochs):\n",
        "    loss_epoch, accuracy_epoch = train(args, train_loader, model2, model3, criterion, optimizer)\n",
        "    print(f\"Epoch [{epoch}/{args.logistic_epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t Accuracy: {accuracy_epoch / len(train_loader)}\")\n",
        "\n",
        "    # final testing\n",
        "    loss_epoch, accuracy_epoch = test(args, test_loader, model2, model3, criterion, optimizer)\n",
        "    print(f\"[FINAL]\\t Loss: {loss_epoch / len(test_loader)}\\t Accuracy: {accuracy_epoch / len(test_loader)}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'boxes': tensor([[[591.7317, 354.3498, 631.2488, 373.1270],\n",
            "         [341.1134, 155.0004, 391.7066, 176.0125],\n",
            "         [546.3565, 497.3262, 585.4752, 514.8645],\n",
            "         [617.9739, 383.6549, 709.0997, 414.2684],\n",
            "         [ 99.9079, 389.4792, 147.2225, 409.4612],\n",
            "         [341.1930, 354.6389, 388.4191, 372.9422],\n",
            "         [597.5522, 496.1530, 648.1851, 515.6973],\n",
            "         [118.3289, 610.1826, 138.0472, 655.3387]]]), 'category': tensor([[2, 2, 2, 5, 2, 2, 2, 2]])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-aca92d453700>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch [{epoch}/{args.logistic_epochs}]\\t Loss: {loss_epoch / len(train_loader)}\\t Accuracy: {accuracy_epoch / len(train_loader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# final testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-ad8e8e2a37d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, loader, model2, model3, criterion, optimizer)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# get encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
          ]
        }
      ]
    }
  ]
}