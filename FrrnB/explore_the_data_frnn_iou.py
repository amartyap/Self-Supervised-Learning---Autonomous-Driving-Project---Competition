# -*- coding: utf-8 -*-
"""explore_the_data_Frnn_IOU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hIqBMSmzPX6d8nSstsJ_6JB-IJIlD1Id

# Final competition of Deep Learning 2020 Spring
Traffic environment semi-supervised Learning Contest

## Goals
The objective is to train a model using images captured by six different cameras attached to the same car to generate a top down view of the surrounding area. The performance of the model will be evaluated by (1) the ability of detecting objects (like car, trucks, bicycles, etc.) and (2) the ability to draw the road map layout.

## Data
You will be given two sets of data:

 1. Unlabeled set: just images
 2. Labeled set: images and the labels(bounding box and road map layout)

This notebook will help you understand the dataset.
"""

#!unzip pytorch-semseg-master.zip

from google.colab import drive
drive.mount('/data/')
from pathlib import Path
base_dir = ('/data/My Drive')

#!cp  /data/My\ Drive/DeepLearning/student_data.zip /content/
#!unzip student_data.zip

import os
import random

import numpy as np
import pandas as pd

import matplotlib
import matplotlib.pyplot as plt
matplotlib.rcParams['figure.figsize'] = [5, 5]
matplotlib.rcParams['figure.dpi'] = 200

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision




import torchvision.transforms as transforms

import sys
#sys.path.append('drive/My Drive/pytorch-semseg-master')
#sys.path.append('drive/My Drive/pytorch-semseg-master/ptsemseg')
from data_helper import UnlabeledDataset, LabeledDataset
from helper import collate_fn, draw_box
from frrn import frrn
#from models

#!pip install pytorch-semseg

#!pip install torchvision==0.5.0

torchvision.__version__

random.seed(0)
np.random.seed(0)
torch.manual_seed(0);

# All the images are saved in image_folder
# All the labels are saved in the annotation_csv file
image_folder = 'data'
annotation_csv = 'data/annotation.csv'

"""# Dataset

You will get two different datasets:

 1. an unlabeled dataset for pre-training
 2. a labeled dataset for both training and validation
 
## The dataset is organized into three levels: scene, sample and image

 1. A scene is 25 seconds of a car's journey.
 2. A sample is a snapshot of a scene at a given timeframe. Each scene will be divided into 126 samples, so about 0.2 seconds between consecutive samples.
 3. Each sample contains 6 images captured by camera facing different orientation.
    Each camera will capture 70 degree view. To make it simple, you can safely assume that the angle between the cameras is 60 degrees 

106 scenes in the unlabeled dataset and 28 scenes in the labeled dataset
"""

# You shouldn't change the unlabeled_scene_index
# The first 106 scenes are unlabeled
unlabeled_scene_index = np.arange(106)
# The scenes from 106 - 133 are labeled
# You should devide the labeled_scene_index into two subsets (training and validation)
labeled_scene_index = np.arange(106, 128)

## validation scene index.
validation_scene_index = np.arange(128, 134)

"""# Unlabeled dataset

You get two ways to access the dataset, by sample or by image

## Get Sample
"""

'''transform = torchvision.transforms.Compose([
    torchvision.transforms.ToPILImage(), # Convert np array to PILImage
    # Resize image to 224 x 224 as required by most vision models
    torchvision.transforms.Resize(
        size=(224, 224)
    ),
    # Convert PIL image to tensor with image values in [0, 1]
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])'''

transform = transforms.Compose([
    #transforms.Resize((224, 224)),
    transforms.ToTensor()
    #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, scene_index=unlabeled_scene_index, first_dim='sample', transform=transform)
trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=3, shuffle=True, num_workers=2)

# [batch_size, 6(images per sample), 3, H, W]
sample = iter(trainloader).next()
print(sample.shape)

# The 6 images orgenized in the following order:
# CAM_FRONT_LEFT, CAM_FRONT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT
plt.imshow(torchvision.utils.make_grid(sample[2], nrow=3).numpy().transpose(1, 2, 0))
plt.axis('off');

"""## Get individual image"""

unlabeled_trainset = UnlabeledDataset(image_folder=image_folder, scene_index=unlabeled_scene_index, first_dim='image', transform=transform)
trainloader = torch.utils.data.DataLoader(unlabeled_trainset, batch_size=2, shuffle=True, num_workers=2)

# [batch_size, 3, H, W]
image, camera_index = iter(trainloader).next()
print(image.shape)

# Camera_index is to tell you which camera is used. The order is
# CAM_FRONT_LEFT, CAM_FRONT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT
print(camera_index[0])

print(camera_index)

plt.imshow(image[0].numpy().transpose(1, 2, 0))
plt.axis('off');

"""# Labeled dataset"""

# The labeled dataset can only be retrieved by sample.
# And all the returned data are tuple of tensors, since bounding boxes may have different size
# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.
labeled_trainset = LabeledDataset(image_folder=image_folder,
                                  annotation_file=annotation_csv,
                                  scene_index=labeled_scene_index,
                                  transform=transform,
                                  extra_info=True
                                 )
trainloader = torch.utils.data.DataLoader(labeled_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)

'''from google.colab import drive
drive.mount('/content/drive')'''

# The labeled dataset can only be retrieved by sample.
# And all the returned data are tuple of tensors, since bounding boxes may have different size
# You can choose whether the loader returns the extra_info. It is optional. You don't have to use it.
validation_trainset = LabeledDataset(image_folder=image_folder,
                                  annotation_file=annotation_csv,
                                  scene_index=validation_scene_index,
                                  transform=transform,
                                  extra_info=True
                                 )
valLoader = torch.utils.data.DataLoader(validation_trainset, batch_size=2, shuffle=True, num_workers=2, collate_fn=collate_fn)

# The 6 images orgenized in the following order:
# CAM_FRONT_LEFT, CAM_FRONT, CAM_FRONT_RIGHT, CAM_BACK_LEFT, CAM_BACK, CAM_BACK_RIGHT
plt.imshow(torchvision.utils.make_grid(sample[0], nrow=3).numpy().transpose(1, 2, 0))
plt.axis('off');

sample, target, road_image, extra = iter(trainloader).next()
print(torch.stack(sample).shape)
print("########")
print(len(target))
print(target[0].keys())
print(target)
print("########")
print(len(road_image))
print(torch.stack(road_image).shape)
print(road_image)
print(road_image[0].size())
print("########")
print(len(extra))
print(extra)
print("########")

sample, target, road_image, extra = iter(valLoader).next()
print(torch.stack(sample).shape)
print("########")
print(len(target))
print(target[0].keys())
print(target)
print("########")
print(len(road_image))
print(torch.stack(road_image).shape)
print(road_image)
print(road_image[0].size())
print("########")
print(len(extra))
print(extra)
print("########")

print(road_image)

print(type(road_image))

print(len(road_image))

print(len(sample))

print(sample[0])

stacked_sample = torch.stack(sample)

sampleNew = stacked_sample.reshape(2,18,256,306)

sampleNew.shape

# size of 1 image.
sampleNew[0].size()

stacked_road_image = torch.stack(road_image)
roadImageNew = stacked_road_image.reshape(2, 800*800)

roadImageNew.shape

roadImageNew.size()

roadImageNew.dtype

print(roadImageNew)

#roadImageNewf = roadImageNew.type(torch.float)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

# build resnet18 model.
'''
import torch
import torch.nn as nn
import torch.nn.functional as F
#from torchvision.models import resnet18
import sys

#nclasses = 800

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.resnet = resnet18()

        self.fc1 = nn.Linear(1000, 100)
        self.fc2 = nn.Linear(100, nclasses)

    def forward(self, x):
        x = self.resnet(x)

        #64x1000
        x = self.fc1(x)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)

        #64x100
        x = self.fc2(x)

        #64x43
        return F.log_softmax(x)'''

#from resnet14 import BasicBlock, Bottleneck, ResNet, ResNet18

#model = ResNet18()
#model.to(device)
#print(model)

def modify_output_for_loss_fn(loss_fn, output, dim):
    if loss_fn == "ce":
        return output
    if loss_fn == "mse":
        return F.softmax(output, dim=dim)
    if loss_fn == "nll":
        return F.log_softmax(output, dim=dim)
    if loss_fn in ["bce", "wbce", "wbce1"]:
        return torch.sigmoid(output)

#model = torchvision.models.resnet18(pretrained=False)

model1=frrn(n_classes=2)

print(model1)

#model.conv1 = nn.Conv2d(18, 64, kernel_size=7, stride=2, padding=3, bias=False)

'''model.fc = torch.nn.Sequential(
    torch.nn.Linear(
        in_features=512,
        out_features=640000
    ),
    torch.nn.Sigmoid()
)'''

model1.classif_conv = torch.nn.Sequential(
    torch.nn.Conv2d(48, 640000, kernel_size=(1, 1), stride=(1, 1)),
    torch.nn.Sigmoid()
)

#print(model1)

#print(model) 
import cv2

import torch.optim as optim
optimizer = optim.Adam(model1.parameters(), lr=0.0001)

from torch.autograd import Variable

def train(epoch):
    total_loss = 0
    model1.train()
    model1.to(device)
    for batch_idx, (sample, target, road_image, extra) in enumerate(trainloader):
        #print(batch_idx)
        #print(len(sample))
        #print(len(road_image))
        #why use torch.stack. lets try using just sample ?
        stacked_sample = torch.stack(sample)
        sampleNew = stacked_sample.reshape(2,18,256,306)
        sampleNew = sampleNew.reshape(2,3,512,918)
        td=np.zeros((2,3,512,1024))
        tr = sampleNew.numpy()
        print(tr.shape)
        for i in range(0,2):
            for j in range(0,3):
                td[i][j] = cv2.resize(tr[i][j], dsize=(1024,512))
        sampleNew = torch.from_numpy(td).float()
        sampleNew = Variable(sampleNew).to(device)
        stacked_roadImage = torch.stack(road_image)
        roadImageNew = stacked_roadImage.reshape(2, 800*800)
        roadImageNewf = roadImageNew.type(torch.float)
        sample, road_image = Variable(sampleNew).to(device), Variable(roadImageNewf).to(device)
        optimizer.zero_grad()
        output = model1(sample)
        
        #print(output.shape)
        #output = torch.sigmoid(output)
        #print(output)
        ##############
        # CHANGE LOSS FUNCTION 
        ##############
        #loss = F.nll_loss(output, road_image)
        #output = F.log_softmax(output)
        loss_func = nn.BCELoss()
    

        loss = loss_func(output,road_image)
        #print(loss)
        loss.backward()
        optimizer.step()
        '''if batch_idx % 1 == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(sample), len(trainloader.dataset),
                100. * batch_idx / len(trainloader), loss.data.item()))'''
        total_loss += loss.item()
    total_loss_div = total_loss/ len(trainloader)
    print("@@@@@@@@@@@@@")
    print(total_loss_div)
    print("@@@@@@@@@@@@@")
    return total_loss

#print(type(sample))
#print(type(road_image))

'''i=1
for epoch in range(1, 5):
    print("Epoch:")
    print(i)
    epoch_loss = train(epoch)
    #print(epoch_loss)
    i += 1'''

def compute_iou(pred, target):
    p = pred.cpu().numpy()
    t = target.cpu().numpy()
    I =   len(np.argwhere(np.logical_and(p==1,t==1)))/len(np.argwhere(np.logical_or(p==1,t==1)))
    #print(I)
    U =   len(np.argwhere(np.logical_and(p==0,t==0)))/len(np.argwhere(np.logical_or(p==0,t==0)))
    #print(U)
    mean_iou = (I+U)/2
    return mean_iou

# build a classifier to predict road image
def validation():
    model1.eval()
    model1.to(device)
    validation_loss = 0
    total_iou = 0
    correct = 0
    #print("*******************")
    with torch.no_grad():
        for batch_idx1, (sample1, target1, road_image1, extra1) in enumerate(valLoader):
            #print(sample1.shape)
            #print(road_image1.shape)
            stacked_sample1 = torch.stack(sample1)
            sampleNew1 = stacked_sample1.reshape(2,18,256,306)
            sampleNew1 = sampleNew1.reshape(2,3,512,918)
            td=np.zeros((2,3,512,1024))
            tr = sampleNew1.numpy()
            for i in range(0,2):
                for j in range(0,3):
                    td[i][j] = cv2.resize(tr[i][j], dsize=(1024, 512))               
            sampleNew1 = torch.from_numpy(td).float()
            sampleNew1 = Variable(sampleNew1).to(device)
            stacked_roadImage1 = torch.stack(road_image1)
            roadImageNew1 = stacked_roadImage1.reshape(2, 800*800)
            roadImageNewf1 = roadImageNew1.type(torch.float)
            sample1, road_image1 = Variable(sampleNew1).to(device), Variable(roadImageNewf1).to(device)
            output1 = model1(sample1)
            #print(torch.unique(outputNew))
            #print(torch.unique(road_image1))
            #output1 = F.log_softmax(output1)
            # validation_loss += F.nll_loss(output, target, size_average=False).data.item() # sum up batch loss
            # loss_func = nn.CrossEntropyLoss()
            loss_func = nn.BCELoss()
            vloss = loss_func(output1, road_image1)
            #print(vloss)
            validation_loss += loss_func(output1, road_image1).data.item() # sum up batch loss
            #print(outputNew.shape)
            #@print(road_image1.shape)
            #p = ((outputNew1 == road_image1).sum().item())
            #print(p)
            #accuracy = p/(outputNew.size()[1]*2)
            outputNew = output1 > 0.5
            outputNew1 = outputNew.type(torch.float)
            iou = compute_iou(outputNew1,road_image1)
            #print(accuracy)
            total_iou += iou
            #pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability
            #correct += pred.eq(target.data.view_as(pred)).cpu().sum()
        #print(len(valLoader))
        validation_loss /= len(valLoader)
        #total_acc /= len(valLoader)
        total_iou /= len(valLoader)
        print("!!!!")
        print('\n IOUy:')
        print(total_iou)
        print("!!!!")
        #print('\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        #    validation_loss, correct, len(validation_loss.dataset),
        #   100. * correct / len(validation_loss.dataset)))
        #print("*******************")
        print("@@@@@@@@@@@@@@@@@@@@")
        print('\nValidation loss: ')
        print(validation_loss)
        print("@@@@@@@@@@@@@@@@@@@@")
        #accuracy = correct / len(validation_loss.dataset) *100
        #return correct, accuracy
        return validation_loss

for epoch in range(0, 2):
    epoch_loss = train(epoch)
    epoch_loss = validation()

# build a classifer for object detection

# build an end to end classifier for both lane image prediction with bounding boxes.

plt.imshow(torchvision.utils.make_grid(sample[0], nrow=3).numpy().transpose(1, 2, 0))
plt.axis('off');

"""There are two kind of labels

 1. The bounding box of surrounding objects
 2. The binary road_image

## Bounding box
"""

# The shape of bounding box is [batch_size, N (the number of object), 2, 4]
print(target[0]['bounding_box'].shape)

# All bounding box are retangles
# Each bounding box is organized with four corners of the box
# All the values are in meter and bounded by 40 meters, and the origin is the center of ego car
# the order of the four courners are front left, front right, back left and back right
print(target[0]['bounding_box'][0])

# Each bounding box has a category
# 'other_vehicle': 0,
# 'bicycle': 1,
# 'car': 2,
# 'pedestrian': 3,
# 'truck': 4,
# 'bus': 5,
# 'motorcycle': 6,
# 'emergency_vehicle': 7,
# 'animal': 8
print(target[0]['category'])

"""## Road Map Layout"""

# The road map layout is encoded into a binary array of size [800, 800] per sample 
# Each pixel is 0.1 meter in physiscal space, so 800 * 800 is 80m * 80m centered at the ego car
# The ego car is located in the center of the map (400, 400) and it is always facing the left

fig, ax = plt.subplots()

ax.imshow(road_image[0], cmap='binary');

print(road_image[0])

"""## Extra Info

There is some extra information you can use in your model, but it is optional.
"""

# Action
# Action is the label that what the object is doing

# 'object_action_parked': 0,
# 'object_action_driving_straight_forward': 1,
# 'object_action_walking': 2,
# 'object_action_running': 3,
# 'object_action_lane_change_right': 4,
# 'object_action_stopped': 5,
# 'object_action_left_turn': 6,
# 'object_action_right_turn': 7,
# 'object_action_sitting': 8,
# 'object_action_standing': 9,
# 'object_action_gliding_on_wheels': 10,
# 'object_action_abnormal_or_traffic_violation': 11,
# 'object_action_lane_change_left': 12,
# 'object_action_other_motion': 13,
# 'object_action_reversing': 14,
# 'object_action_u_turn': 15,
# 'object_action_loss_of_control': 16

print(extra[0].keys())

print(extra[0]['action'])

# Ego Image
# A more detailed ego image
fig, ax = plt.subplots()

ax.imshow(extra[0]['ego_image'].numpy().transpose(1, 2, 0));

extra[0]['ego_image']

np.unique(extra[0]['ego_image'].numpy())

np.unique(extra[0]['lane_image'].numpy())

np.unique(road_image[0].numpy())

# Lane Image
# Binary lane image
fig, ax = plt.subplots()

ax.imshow(extra[0]['lane_image'], cmap='binary');

extra[0]['lane_image']

"""# Visualize the bounding box"""

# The center of image is 400 * 400

fig, ax = plt.subplots()

color_list = ['b', 'g', 'orange', 'c', 'm', 'y', 'k', 'w', 'r']

ax.imshow(road_image[0], cmap ='binary');

# The ego car position
ax.plot(400, 400, 'x', color="red")

for i, bb in enumerate(target[0]['bounding_box']):
    # You can check the implementation of the draw box to understand how it works 
    draw_box(ax, bb, color=color_list[target[0]['category'][i]])

"""# Evaluation
During the whole competition, you have three submission deadlines. The dates will be announced on Piazza. You will have to fill up the template 'data_loader.py' for evaluation. (see the comment inside data_loader.py' for more information)

There will be two leaderboards for the competition:
The leaderboard for binary road map.
We will evaluate your model's performance by using the average threat score (TS) across the test set:
$$\text{TS} = \frac{\text{TP}}{\text{TP} + \text{FP} + \text{FN}}$$
The leaderboard for object detection:
We will evaluate your model's performance for object detection by using the average mean threat score at different intersection over union (IoU) thresholds.
There will be five different thresholds (0.5, 0.6, 0.7, 0.8, 0.9). For each thresholds, we will calculate the threat score. The final score will be a weighted average of all the threat scores:
$$\text{Final Score} = \sum_t \frac{1}{t} \cdot \frac{\text{TP}(t)}{\text{TP}(t) + \text{FP}(t) + \text{FN}(t)}$$
"""

